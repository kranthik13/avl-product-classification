{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kranthi/anaconda3/lib/python3.7/site-packages/tqdm/std.py:656: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "tqdm.pandas()\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "pd.options.display.max_colwidth = 200\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import unidecode\n",
    "import codecs\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    train = pd.read_csv(\"../data/v2/train_10_skf.csv\")\n",
    "    test = pd.read_csv(\"../data/test.csv\", sep=\"\\t\")\n",
    "    \n",
    "    print(\"Train Shape : {}\\nTest Shape :  {}\".format(train.shape, test.shape))\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape : (1199998, 5)\n",
      "Test Shape :  (92, 3)\n"
     ]
    }
   ],
   "source": [
    "train, test = get_data()\n",
    "target = 'category'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>category</th>\n",
       "      <th>fold_id</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZicZac // Black + Red (Euro: 44)</td>\n",
       "      <td>Clothing &amp; related products (B2C) - Shoes and shoe laces</td>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>valid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9X9 RESISTA/484938</td>\n",
       "      <td>Publishing/Printing - Printing Services</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Halle Pant - Short Inseam 013049561D0010001_ 02</td>\n",
       "      <td>Clothing &amp; related products (B2C) - General</td>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             title  \\\n",
       "0                 ZicZac // Black + Red (Euro: 44)   \n",
       "1                               9X9 RESISTA/484938   \n",
       "2  Halle Pant - Short Inseam 013049561D0010001_ 02   \n",
       "\n",
       "                                                description category  fold_id  \\\n",
       "0  Clothing & related products (B2C) - Shoes and shoe laces        R        1   \n",
       "1                   Publishing/Printing - Printing Services        S        1   \n",
       "2               Clothing & related products (B2C) - General        R        1   \n",
       "\n",
       "  source  \n",
       "0  valid  \n",
       "1  train  \n",
       "2  train  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO : LATER\n",
    "\n",
    "import the imputed dataset for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.fillna(\"none\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning  (For frequency-methods): \n",
    "\n",
    "Usually this type of **regular text cleaning** could be useful while using frequency methods and own-embedding models but when using pre-trained methods we have to preprocess differently(shown later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps : \n",
    "\n",
    "1. Lower\n",
    "1. Don't split camelcase\n",
    "1. Dealing with contractions\n",
    "1. removing special characters\n",
    "1. removing stop-words\n",
    "1. lemmatization ? \n",
    "1. tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\",  \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\",\"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\",\"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def treat_text(X):\n",
    "    # Decoding \n",
    "    try:\n",
    "        decoded = unidecode.unidecode(codecs.decode(X, 'unicode_escape'))\n",
    "    except:\n",
    "        decoded = unidecode.unidecode(X)\n",
    "\n",
    "    # Handling apostrophes\n",
    "    apostrophe_handled = re.sub(\"’\", \"'\", decoded)\n",
    "    X = \" \".join([contraction_mapping[t] if t in contraction_mapping else t for t in apostrophe_handled.split(\" \")])\n",
    "    \n",
    "    # Keeping only text + numbers and lowered.\n",
    "    X = re.findall(r\"[a-zA-Z0-9]+\", X.lower())\n",
    "    \n",
    "    # Removing stopwords\n",
    "    X = [word for word in X if (word not in stop_words)]\n",
    "    \n",
    "    # Lemming\n",
    "#     X = [wnl.lemmatize(word) for word in X]\n",
    "    \n",
    "    return \" \".join(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Preprocessing \n",
      "\n",
      "Original Text : ZicZac // Black + Red (Euro: 44)\n",
      "Preprocessed Text : ziczac black red euro 44\n",
      "\n",
      "Original Text : 9X9 RESISTA/484938\n",
      "Preprocessed Text : 9x9 resista 484938\n",
      "\n",
      "Original Text : Halle Pant - Short Inseam 013049561D0010001_ 02\n",
      "Preprocessed Text : halle pant short inseam 013049561d0010001 02\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample Preprocessing \\n\")\n",
    "for i in range(3):\n",
    "    item = train['title'].iloc[i]\n",
    "    print(\"Original Text : {}\\nPreprocessed Text : {}\\n\".format(item, treat_text(item)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1199998/1199998 [00:22<00:00, 54194.49it/s]\n",
      "100%|██████████| 1199998/1199998 [00:19<00:00, 61314.93it/s]\n",
      "100%|██████████| 92/92 [00:00<00:00, 44180.90it/s]\n",
      "100%|██████████| 92/92 [00:00<00:00, 18130.71it/s]\n"
     ]
    }
   ],
   "source": [
    "for data in [train, test]:\n",
    "    data['title'] = data['title'].progress_apply(lambda x: treat_text(x))\n",
    "    data['description'] = data['description'].progress_apply(lambda x: treat_text(x))\n",
    "    data['text'] = data['title'] + \" \" + data['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>category</th>\n",
       "      <th>fold_id</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ziczac black red euro 44</td>\n",
       "      <td>clothing related products b2c shoes shoe laces</td>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>valid</td>\n",
       "      <td>ziczac black red euro 44 clothing related products b2c shoes shoe laces</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9x9 resista 484938</td>\n",
       "      <td>publishing printing printing services</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>9x9 resista 484938 publishing printing printing services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>halle pant short inseam 013049561d0010001 02</td>\n",
       "      <td>clothing related products b2c general</td>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>halle pant short inseam 013049561d0010001 02 clothing related products b2c general</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>harry houser travel expenses meals</td>\n",
       "      <td>security personnel</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>harry houser travel expenses meals security personnel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tee time 740078609 greens fee composite</td>\n",
       "      <td>admissions green fees privately owned golf course</td>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>valid</td>\n",
       "      <td>tee time 740078609 greens fee composite admissions green fees privately owned golf course</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          title  \\\n",
       "0                      ziczac black red euro 44   \n",
       "1                            9x9 resista 484938   \n",
       "2  halle pant short inseam 013049561d0010001 02   \n",
       "3            harry houser travel expenses meals   \n",
       "4       tee time 740078609 greens fee composite   \n",
       "\n",
       "                                         description category  fold_id source  \\\n",
       "0     clothing related products b2c shoes shoe laces        R        1  valid   \n",
       "1              publishing printing printing services        S        1  train   \n",
       "2              clothing related products b2c general        R        1  train   \n",
       "3                                 security personnel        S        1  train   \n",
       "4  admissions green fees privately owned golf course        R        1  valid   \n",
       "\n",
       "                                                                                        text  \n",
       "0                    ziczac black red euro 44 clothing related products b2c shoes shoe laces  \n",
       "1                                   9x9 resista 484938 publishing printing printing services  \n",
       "2         halle pant short inseam 013049561d0010001 02 clothing related products b2c general  \n",
       "3                                      harry houser travel expenses meals security personnel  \n",
       "4  tee time 740078609 greens fee composite admissions green fees privately owned golf course  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"../data/v2/train_ne.csv\", index=False)\n",
    "test.to_csv(\"../data/v2/test_ne.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train, test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing : (For Embedding)\n",
    "\n",
    "I am using google-news-vectors to show how the preprocessing for pre-trained embeddings is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences, verbose =  True):\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences, disable = (not verbose)):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-initialising the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape : (1199998, 4)\n",
      "Test Shape :  (92, 3)\n"
     ]
    }
   ],
   "source": [
    "train, test = get_data()\n",
    "\n",
    "test.fillna(\"none\", inplace=True)\n",
    "\n",
    "train['text'] = train['title'] + \" \" + train['description']\n",
    "test['text'] = test['title'] + \" \" + test['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "\n",
    "def tokenize(s): \n",
    "    return re_tok.sub(r' \\1 ', s).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1199998/1199998 [00:35<00:00, 33559.24it/s]\n",
      "100%|██████████| 92/92 [00:00<00:00, 21777.53it/s]\n",
      "100%|██████████| 1199998/1199998 [00:09<00:00, 128000.74it/s]\n",
      "100%|██████████| 92/92 [00:00<00:00, 43611.66it/s]\n"
     ]
    }
   ],
   "source": [
    "train_sentences = train['text'].progress_apply(lambda x: tokenize(x)).values\n",
    "test_sentences = test['text'].progress_apply(lambda x: tokenize(x)).values\n",
    "\n",
    "train_vocab = build_vocab(train_sentences)\n",
    "test_vocab = build_vocab(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ZicZac': 1, '/': 1031538, 'Black': 33463, '+': 18240, 'Red': 4638}\n"
     ]
    }
   ],
   "source": [
    "# Sample vocabulary\n",
    "print({k: train_vocab[k] for k in list(train_vocab)[: 5]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the pre-trained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.5 s, sys: 10.3 s, total: 46.9 s\n",
      "Wall time: 46.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "news_path = '../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin'\n",
    "embeddings_index = KeyedVectors.load_word2vec_format(news_path, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OOV : out of vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def check_coverage(vocab, embedding_index):\n",
    "    intersection = {}\n",
    "    oov = {}\n",
    "    found_len = 0\n",
    "    not_found_len = 0\n",
    "    \n",
    "    for word in tqdm(vocab):\n",
    "        try:\n",
    "            intersection[word] = embedding_index[word]\n",
    "            found_len += vocab[word]\n",
    "        except:\n",
    "            oov[word] = vocab[word]\n",
    "            not_found_len += vocab[word]\n",
    "            \n",
    "    print(\"Found embeddings for {:.2f} % of vocab\".format((len(intersection) / len(vocab))*100))\n",
    "    print(\"Found embeddings for {:.2f} % of all text\".format((found_len / (found_len + not_found_len))*100))\n",
    "    \n",
    "    sorted_oov = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "    \n",
    "    return sorted_oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 19606/1208022 [00:00<00:06, 196056.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1208022/1208022 [00:05<00:00, 230316.86it/s]\n",
      "100%|██████████| 949/949 [00:00<00:00, 161116.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 8.99 % of vocab\n",
      "Found embeddings for 57.38 % of all text\n",
      "Test\n",
      "Found embeddings for 93.47 % of vocab\n",
      "Found embeddings for 74.12 % of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Train\")\n",
    "train_oov = check_coverage(train_vocab, embeddings_index)\n",
    "print(\"Test\")\n",
    "test_oov = check_coverage(test_vocab, embeddings_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's understand the terms : \n",
    "\n",
    "1. Vocab Coverage : This is the percentage of unique tokens/words found in our pre-trained embeddings which matches with our own vocab.\n",
    "\n",
    "![Vocab Coverage](https://latex.codecogs.com/gif.latex?VocabCoverage%20%3D%20\\frac{Matching%20Terms%20In%20Pretrained%20Embedding}{TotalTermsInOurVocab})\n",
    "\n",
    "2. All Text Coverage : This is the percentage of total tokens matched by the total number of tokens we have.\n",
    "\n",
    "![All Text Coverage](https://latex.codecogs.com/gif.latex?AllTextCoverage%20%3D%20%5Cfrac%7BMatchedTokensInOurVocab%7D%7BTotalTokensInOurVocab%7D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our main aim now is to increase the vocab by understanding how the underlying pre-trained embedding is trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('-', 2208672),\n",
       " ('/', 1031538),\n",
       " ('(', 927001),\n",
       " (':', 926898),\n",
       " (')', 925636),\n",
       " ('.', 476453),\n",
       " (',', 332092),\n",
       " ('of', 165727),\n",
       " ('and', 146635),\n",
       " ('2019', 83006)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_oov[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punctuations seems to be a problem here. Let's fix that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'?' in embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'&' in embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(x):\n",
    "\n",
    "    x = str(x)\n",
    "    for punct in \"/-'\":\n",
    "        x = x.replace(punct, ' ')\n",
    "    for punct in '&':\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
    "        x = x.replace(punct, '')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1199998/1199998 [00:17<00:00, 68887.66it/s]\n",
      "100%|██████████| 92/92 [00:00<00:00, 30047.97it/s]\n",
      "100%|██████████| 1199998/1199998 [00:15<00:00, 77716.92it/s]\n",
      "100%|██████████| 92/92 [00:00<00:00, 34060.90it/s]\n",
      "100%|██████████| 1199998/1199998 [00:08<00:00, 137588.34it/s]\n",
      "100%|██████████| 92/92 [00:00<00:00, 47457.38it/s]\n",
      "100%|██████████| 1277988/1277988 [00:05<00:00, 232069.82it/s]\n",
      "100%|██████████| 932/932 [00:00<00:00, 189009.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 8.26 % of vocab\n",
      "Found embeddings for 79.29 % of all text\n",
      "Found embeddings for 93.56 % of vocab\n",
      "Found embeddings for 89.69 % of all text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train[\"text\"] = train[\"text\"].progress_apply(lambda x: clean_text(x))\n",
    "test[\"text\"] = test[\"text\"].progress_apply(lambda x: clean_text(x))\n",
    "\n",
    "train_sentences = train[\"text\"].progress_apply(lambda x: tokenize(x))\n",
    "test_sentences = test[\"text\"].progress_apply(lambda x: tokenize(x))\n",
    "train_vocab = build_vocab(train_sentences)\n",
    "test_vocab = build_vocab(test_sentences)\n",
    "\n",
    "oov = check_coverage(train_vocab, embeddings_index)\n",
    "oov_test = check_coverage(test_vocab, embeddings_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the stopwords : [of, and, to] and numbers > 10 were masked to \"##\".\n",
    "\n",
    "Let's fix this.\n",
    "\n",
    "But fixing the numbers may ot be good as we saw in EDA notebook that most *product* categories have sizes in numbers like for shirts the size could be [42, 40], etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>category</th>\n",
       "      <th>fold_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZicZac // Black + Red (Euro: 44)</td>\n",
       "      <td>Clothing &amp; related products (B2C) - Shoes and shoe laces</td>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>ZicZac    Black  Red Euro 44 Clothing  &amp;  related products B2C   Shoes and shoe laces</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9X9 RESISTA/484938</td>\n",
       "      <td>Publishing/Printing - Printing Services</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>9X9 RESISTA 484938 Publishing Printing   Printing Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Halle Pant - Short Inseam 013049561D0010001_ 02</td>\n",
       "      <td>Clothing &amp; related products (B2C) - General</td>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>Halle Pant   Short Inseam 013049561D0010001 02 Clothing  &amp;  related products B2C   General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Houser Travel Expenses - Meals</td>\n",
       "      <td>Security - personnel</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>Harry Houser Travel Expenses   Meals Security   personnel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tee Time: 740078609 : Greens Fee - Composite</td>\n",
       "      <td>Admissions - Green Fees for Privately Owned Golf Course</td>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>Tee Time 740078609  Greens Fee   Composite Admissions   Green Fees for Privately Owned Golf Course</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             title  \\\n",
       "0                 ZicZac // Black + Red (Euro: 44)   \n",
       "1                               9X9 RESISTA/484938   \n",
       "2  Halle Pant - Short Inseam 013049561D0010001_ 02   \n",
       "3             Harry Houser Travel Expenses - Meals   \n",
       "4     Tee Time: 740078609 : Greens Fee - Composite   \n",
       "\n",
       "                                                description category  fold_id  \\\n",
       "0  Clothing & related products (B2C) - Shoes and shoe laces        R        1   \n",
       "1                   Publishing/Printing - Printing Services        S        1   \n",
       "2               Clothing & related products (B2C) - General        R        1   \n",
       "3                                      Security - personnel        S        1   \n",
       "4   Admissions - Green Fees for Privately Owned Golf Course        R        1   \n",
       "\n",
       "                                                                                                 text  \n",
       "0               ZicZac    Black  Red Euro 44 Clothing  &  related products B2C   Shoes and shoe laces  \n",
       "1                                          9X9 RESISTA 484938 Publishing Printing   Printing Services  \n",
       "2          Halle Pant   Short Inseam 013049561D0010001 02 Clothing  &  related products B2C   General  \n",
       "3                                           Harry Houser Travel Expenses   Meals Security   personnel  \n",
       "4  Tee Time 740078609  Greens Fee   Composite Admissions   Green Fees for Privately Owned Golf Course  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.to_csv(\"train_w2v.csv\", index=False)\n",
    "test.to_csv(\"test_w2v.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO : \n",
    "\n",
    "1. Handle Contractions\n",
    "1. Handle  Word-Corrections\n",
    "1. This embeddings is trained on US english so most of the common words could be converted into UK english \n",
    "        For example : the keywords [Instagram, Facebook, etc] are mapped to [social media]\n",
    "1. Can be done a very-time consuming coverage increase (below) but I didn't use this I think instead of pre-trained embedding a custom-trained embedding would proved benefical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
