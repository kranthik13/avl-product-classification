{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import ast\n",
    "import csv\n",
    "import sys \n",
    "import time\n",
    "import codecs\n",
    "import string\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GroupShuffleSplit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "# Classification models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cat\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Bidirectional, Dense, LSTM, Embedding, Dropout, Activation, Lambda\n",
    "from keras.layers import Add, Flatten, BatchNormalization, concatenate, SpatialDropout1D\n",
    "from keras.layers import Conv1D, GlobalMaxPool1D, GlobalAveragePooling1D, GlobalMaxPooling1D, MaxPooling1D, MaxPool1D\n",
    "from keras.models import Model, load_model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras import backend as Ke\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "tqdm.pandas()\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.display.max_colwidth = 200\n",
    "pd.options.display.max_rows = 100\n",
    "\n",
    "start_time = time.time()\n",
    "np.random.seed(32)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"4\"\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import unidecode\n",
    "import codecs\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    train = pd.read_csv(\"/kaggle/input/avalaraproduct-classification/train.tsv\", sep=\"\\t\")\n",
    "    test = pd.read_csv(\"/kaggle/input/avalaraproduct-classification/test.csv\", sep=\"\\t\")\n",
    "    \n",
    "    print(\"Train Shape : {}\\nTest Shape :  {}\".format(train.shape, test.shape))\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape : (1200000, 3)\n",
      "Test Shape :  (92, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZicZac // Black + Red (Euro: 44)</td>\n",
       "      <td>Clothing &amp; related products (B2C) - Shoes and shoe laces</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9X9 RESISTA/484938</td>\n",
       "      <td>Publishing/Printing - Printing Services</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Halle Pant - Short Inseam 013049561D0010001_ 02</td>\n",
       "      <td>Clothing &amp; related products (B2C) - General</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Houser Travel Expenses - Meals</td>\n",
       "      <td>Security - personnel</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tee Time: 740078609 : Greens Fee - Composite</td>\n",
       "      <td>Admissions - Green Fees for Privately Owned Golf Course</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             title  \\\n",
       "0                 ZicZac // Black + Red (Euro: 44)   \n",
       "1                               9X9 RESISTA/484938   \n",
       "2  Halle Pant - Short Inseam 013049561D0010001_ 02   \n",
       "3             Harry Houser Travel Expenses - Meals   \n",
       "4     Tee Time: 740078609 : Greens Fee - Composite   \n",
       "\n",
       "                                                description category  \n",
       "0  Clothing & related products (B2C) - Shoes and shoe laces        R  \n",
       "1                   Publishing/Printing - Printing Services        S  \n",
       "2               Clothing & related products (B2C) - General        R  \n",
       "3                                      Security - personnel        S  \n",
       "4   Admissions - Green Fees for Privately Owned Golf Course        R  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = get_data()\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split : 1\n",
      "------------------------------------------------------------\n",
      "Split : 2\n",
      "------------------------------------------------------------\n",
      "Split : 3\n",
      "------------------------------------------------------------\n",
      "Split : 4\n",
      "------------------------------------------------------------\n",
      "Split : 5\n",
      "------------------------------------------------------------\n",
      "Split : 6\n",
      "------------------------------------------------------------\n",
      "Split : 7\n",
      "------------------------------------------------------------\n",
      "Split : 8\n",
      "------------------------------------------------------------\n",
      "Split : 9\n",
      "------------------------------------------------------------\n",
      "Split : 10\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4523409f7d24d64bc14a3ec50839ec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=10, random_state=13)\n",
    "\n",
    "train['fold_id'] = np.nan\n",
    "\n",
    "for i, (trn_idx, val_idx) in enumerate(skf.split(train, train['category']), 1):\n",
    "    print(\"Split : {}\".format(i))\n",
    "    train['fold_id'].iloc[val_idx] = i\n",
    "    print(\"--\"*30)\n",
    "train['fold_id'] = train['fold_id'].astype(np.int)\n",
    "\n",
    "train['source'] = np.nan\n",
    "\n",
    "new_df = pd.DataFrame()\n",
    "\n",
    "for i in tqdm_notebook(train['fold_id'].unique()):\n",
    "#     print(i)\n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=13)\n",
    "    train_subsample = train[train['fold_id'] == i].copy()\n",
    "    train_subsample.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    for idx, (trn_idx, val_idx) in enumerate(gss.split(\n",
    "        X=train_subsample, y=train_subsample['category'], groups=train_subsample['description'])):\n",
    "        \n",
    "        train_set = train_subsample.iloc[trn_idx]\n",
    "        valid_set = train_subsample.iloc[val_idx]\n",
    "\n",
    "        train_subsample['source'].iloc[trn_idx] = \"train\"\n",
    "        train_subsample['source'].iloc[val_idx] = \"valid\"\n",
    "        \n",
    "        print(len(set.intersection(set(train_set['description'].values), set(valid_set['description'].values))))\n",
    "    new_df = pd.concat([new_df, train_subsample], axis=0)\n",
    "    new_df.reset_index(drop=True, inplace=True)\n",
    "#     print(new_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title          0\n",
       "description    0\n",
       "category       0\n",
       "fold_id        0\n",
       "source         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = new_df.copy()\n",
    "del new_df\n",
    "gc.collect()\n",
    "\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"train_10_skf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    train = pd.read_csv(\"train_10_skf.csv\")\n",
    "    test = pd.read_csv(\"/kaggle/input/avalaraproduct-classification/test.csv\", sep=\"\\t\")\n",
    "    \n",
    "    print(\"Train Shape : {}\\nTest Shape :  {}\".format(train.shape, test.shape))\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.fillna(\"none\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\",  \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\",\"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\",\"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def treat_text(X):\n",
    "    # Decoding \n",
    "    try:\n",
    "        decoded = unidecode.unidecode(codecs.decode(X, 'unicode_escape'))\n",
    "    except:\n",
    "        decoded = unidecode.unidecode(X)\n",
    "\n",
    "    # Handling apostrophes\n",
    "    apostrophe_handled = re.sub(\"’\", \"'\", decoded)\n",
    "    X = \" \".join([contraction_mapping[t] if t in contraction_mapping else t for t in apostrophe_handled.split(\" \")])\n",
    "    \n",
    "    # Keeping only text + numbers and lowered.\n",
    "    X = re.findall(r\"[a-zA-Z0-9]+\", X.lower())\n",
    "    \n",
    "    # Removing stopwords\n",
    "    X = [word for word in X if (word not in stop_words)]\n",
    "    \n",
    "    # Lemming\n",
    "#     X = [wnl.lemmatize(word) for word in X]\n",
    "    \n",
    "    return \" \".join(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Preprocessing \n",
      "\n",
      "Original Text : ZicZac // Black + Red (Euro: 44)\n",
      "Preprocessed Text : ziczac black red euro 44\n",
      "\n",
      "Original Text : 9X9 RESISTA/484938\n",
      "Preprocessed Text : 9x9 resista 484938\n",
      "\n",
      "Original Text : Halle Pant - Short Inseam 013049561D0010001_ 02\n",
      "Preprocessed Text : halle pant short inseam 013049561d0010001 02\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample Preprocessing \\n\")\n",
    "for i in range(3):\n",
    "    item = train['title'].iloc[i]\n",
    "    print(\"Original Text : {}\\nPreprocessed Text : {}\\n\".format(item, treat_text(item)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1199998/1199998 [00:21<00:00, 57132.17it/s]\n",
      "100%|██████████| 1199998/1199998 [00:18<00:00, 64068.33it/s]\n",
      "100%|██████████| 92/92 [00:00<00:00, 36293.83it/s]\n",
      "100%|██████████| 92/92 [00:00<00:00, 21460.21it/s]\n"
     ]
    }
   ],
   "source": [
    "for data in [train, test]:\n",
    "    data['title'] = data['title'].progress_apply(lambda x: treat_text(x))\n",
    "    data['description'] = data['description'].progress_apply(lambda x: treat_text(x))\n",
    "    data['text'] = data['title'] + \" \" + data['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"train_ne.csv\", index=False)\n",
    "test.to_csv(\"test_ne.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title          0\n",
       "description    0\n",
       "category       0\n",
       "fold_id        0\n",
       "source         0\n",
       "text           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'category'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['description'][test['description'] == \"none\"] = np.nan\n",
    "test['description'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_vec(words, model, num_features):\n",
    "    \"\"\"\n",
    "    Average the word vectors for a set of words\n",
    "    \"\"\"\n",
    "    feature_vec = np.zeros((num_features,),dtype=\"float32\")  # pre-initialize (for speed)\n",
    "    nwords = 0.\n",
    "    index2word_set = set(model.wv.index2word)  # words known to the model\n",
    "\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "    \n",
    "    feature_vec = np.divide(feature_vec, nwords)\n",
    "    return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1199998/1199998 [00:05<00:00, 229421.86it/s]\n",
      "100%|██████████| 92/92 [00:00<00:00, 48210.39it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(120000, 7)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['title_tokenized'] = train['title'].progress_apply(lambda x: x.split())\n",
    "test['title_tokenized'] = test['title'].progress_apply(lambda x: x.split())\n",
    "\n",
    "train_subsample = train[train['fold_id'] == K]\n",
    "train_subsample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Vocabulary : 5820\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('men', 0.9801139831542969),\n",
       " ('polo', 0.9693353176116943),\n",
       " ('long', 0.9682233333587646),\n",
       " ('jacket', 0.9652382135391235),\n",
       " ('sweatshirt', 0.9651938080787659),\n",
       " ('women', 0.961998701095581),\n",
       " ('hoodie', 0.9617142081260681),\n",
       " ('button', 0.9614996314048767),\n",
       " ('ladies', 0.9599067568778992),\n",
       " ('sleeve', 0.9585515856742859)]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = 'title'\n",
    "values = train_subsample['{}_tokenized'.format(col)].values.tolist()\n",
    "model = Word2Vec(values, min_count=10)\n",
    "print(\"Length of Vocabulary : {}\".format(len(model.wv.vocab)))\n",
    "\n",
    "model.most_similar(\"shirt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f00922669191432898ce9900dc970b6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=120000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_sentence_embs = np.zeros((train_subsample.shape[0], 100))\n",
    "\n",
    "for i in tqdm_notebook(range(train_subsample.shape[0])):\n",
    "    train_sentence_embs[i] = make_feature_vec(train['title_tokenized'].iloc[i], model, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c66286b64e489eba66d13a148c863f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=92.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_sentence_embs = np.zeros((test.shape[0], 100))\n",
    "\n",
    "for i in tqdm_notebook(range(test.shape[0])):\n",
    "    test_sentence_embs[i] = make_feature_vec(test['title_tokenized'].iloc[i], model, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cosine_similarity(train_vecs, test_vec):\n",
    "    start_time = time.time()\n",
    "    result = np.full(len(train_vecs), -9999, dtype=float)\n",
    "    for idx, _ in enumerate(range(len(train_vecs))):\n",
    "        r = 1 - cosine(train_vecs[idx], test_vec)\n",
    "        if str(r) != \"nan\":\n",
    "            result[idx] = r\n",
    "    \n",
    "    print(\"Time Taken : {:.2f}\".format(time.time() - start_time))\n",
    "    \n",
    "    return np.argmax(result), result[np.argmax(result)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Taken : 7.56\n",
      "Index : 44706 \t Similarity : 0.9829737911680743\n",
      "Test Item : carpet repairs\n",
      "Train Title Item matched : cust shorted ctrl board trying repl batt\n",
      "Train Description mapped : repair performed tpp equipment parts labor separately stated\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Time Taken : 7.38\n",
      "Index : 4611 \t Similarity : 0.9740908401710531\n",
      "Test Item : vct floor refinishing\n",
      "Train Title Item matched : fs436 us26d floor stop\n",
      "Train Description mapped : hardware sold medical facility\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Time Taken : 7.19\n",
      "Index : 43582 \t Similarity : 0.9005709003919636\n",
      "Test Item : clean carpet clean windows scrub buff vct floors\n",
      "Train Title Item matched : front porch actor windows ba\n",
      "Train Description mapped : computer software implementation prewritten software electronically downloaded\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Time Taken : 7.25\n",
      "Index : 28269 \t Similarity : 0.9788033306624719\n",
      "Test Item : apparel customization alterations\n",
      "Train Title Item matched : lv ur68 fu2z\n",
      "Train Description mapped : clothing related products b2c general\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Time Taken : 8.09\n",
      "Index : 24563 \t Similarity : 0.9783133435038731\n",
      "Test Item : auto scrub coat lvt wax tile floors\n",
      "Train Title Item matched : face mount auto belay bracket\n",
      "Train Description mapped : installation associated sale tpp equipment parts labor separately stated\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Time Taken : 7.40\n",
      "Index : 56356 \t Similarity : 0.9930445291121084\n",
      "Test Item : extension labor services\n",
      "Train Title Item matched : remote combustion tuning services labor\n",
      "Train Description mapped : professional services\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Time Taken : 6.99\n",
      "Index : 28269 \t Similarity : 0.9849425885196562\n",
      "Test Item : apparel uniform accessories\n",
      "Train Title Item matched : lv ur68 fu2z\n",
      "Train Description mapped : clothing related products b2c general\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Time Taken : 7.12\n",
      "Index : 88189 \t Similarity : 0.9659217630556148\n",
      "Test Item : window cleaning\n",
      "Train Title Item matched : 1602 veterans blvd window cleaning mccomb ms\n",
      "Train Description mapped : janitorial non residential\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Time Taken : 7.08\n",
      "Index : 67140 \t Similarity : 0.968434504110343\n",
      "Test Item : bathroom vestibule ssw santizing\n",
      "Train Title Item matched : iarw trash tuesday thursday\n",
      "Train Description mapped : services\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Time Taken : 7.44\n",
      "Index : 82660 \t Similarity : 0.9620837093339802\n",
      "Test Item : store cleaning\n",
      "Train Title Item matched : emergency response cleaning urine\n",
      "Train Description mapped : janitorial non residential\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Time Taken : 7.21\n",
      "Index : 44001 \t Similarity : 0.9832836114759136\n",
      "Test Item : carpet laminate wood floors\n",
      "Train Title Item matched : 16533 mount laurel nj\n",
      "Train Description mapped : janitorial non residential\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Time Taken : 6.92\n",
      "Index : 51093 \t Similarity : 1.0\n",
      "Test Item : janitorial services\n",
      "Train Title Item matched : may2018 janitorial services tmc045\n",
      "Train Description mapped : janitorial non residential\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Time Taken : 7.63\n",
      "Index : 97989 \t Similarity : 0.9584161350822471\n",
      "Test Item : apparel patches\n",
      "Train Title Item matched : ice display pedestal ret\n",
      "Train Description mapped : tangible personal property tpp\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Time Taken : 7.53\n",
      "Index : 3962 \t Similarity : 1.0\n",
      "Test Item : service\n",
      "Train Title Item matched : conveyor service\n",
      "Train Description mapped : installation associated sale tpp equipment parts labor separately stated\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Time Taken : 7.18\n",
      "Index : 73964 \t Similarity : 0.9574252416730645\n",
      "Test Item : repair maintenance services performed tpp include parts materials labor\n",
      "Train Title Item matched : completion repair services pricing parts applied separately replaced temple 2342 18\n",
      "Train Description mapped : repair performed tpp labor\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Time Taken : 7.13\n",
      "Index : 80648 \t Similarity : 0.9932654795654583\n",
      "Test Item : restorative carpet cleaning\n",
      "Train Title Item matched : carpet upholstery cleaning k4 1306 4477033780\n",
      "Train Description mapped : janitorial non residential\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "null_idxs = test[test['description'].isnull()].index.tolist()\n",
    "\n",
    "mapping = {}\n",
    "\n",
    "for i in null_idxs:\n",
    "    item = test['title'].iloc[i]\n",
    "    mapping[i] = {}\n",
    "    mapping[i]['test_title'] = item\n",
    "    \n",
    "    max_idx, max_sim = calc_cosine_similarity(train_sentence_embs, test_sentence_embs[i])\n",
    "    print(\"Index : {} \\t Similarity : {}\".format(max_idx, max_sim))\n",
    "\n",
    "    mapping[i]['train_title'] = train_subsample['title'].iloc[max_idx]\n",
    "    mapping[i]['mapped_desc'] = train_subsample['description'].iloc[max_idx]\n",
    "    \n",
    "    print(\"Test Item : {}\".format(item))\n",
    "    print(\"Train Title Item matched : {}\".format(mapping[i]['train_title']))\n",
    "    print(\"Train Description mapped : {}\".format(mapping[i]['mapped_desc']))\n",
    "    print(\"--\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in mapping.items():\n",
    "    test['description'].iloc[key] = value['mapped_desc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title              0\n",
       "description        0\n",
       "category           0\n",
       "text               0\n",
       "title_tokenized    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['text'] = test['title'] + \" \" + test['description']\n",
    "test.to_csv(\"test_ne_imputed.csv\", index=False)\n",
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling - 1 : Frequency Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train, test):\n",
    "    train = pd.read_csv(\"train_ne.csv\")\n",
    "    test = pd.read_csv(\"test_ne_imputed.csv\")\n",
    "    print(\"Train Shape : {}\\nTest Shape :  {}\".format(train.shape, test.shape))\n",
    "    \n",
    "    train = train[['fold_id', 'title', 'description', 'text', 'category', 'source']]\n",
    "    test = test[['title', 'description', 'text', 'category']]\n",
    "    train.dropna(inplace=True)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape : (1199998, 6)\n",
      "Test Shape :  (92, 5)\n"
     ]
    }
   ],
   "source": [
    "train, test = get_data(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labelling Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['R' 'S']\n"
     ]
    }
   ],
   "source": [
    "target = 'category'\n",
    "\n",
    "le = LabelEncoder()\n",
    "train[target] = le.fit_transform(train[target].values)\n",
    "test[target] = le.transform(test[target].values)\n",
    "\n",
    "y = train[target].values\n",
    "y_test = test[target].values\n",
    "\n",
    "print(le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitter(data, k=K):\n",
    "    \n",
    "    data_v1 = data[data['fold_id'] == k].copy()\n",
    "    data_v1.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    train_set = data_v1[data_v1['source'] == 'train'].copy()\n",
    "    valid_set = data_v1[data_v1['source'] == 'valid'].copy()\n",
    "    train_set.reset_index(drop=True, inplace=True)\n",
    "    valid_set.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    del data_v1\n",
    "    gc.collect()\n",
    "    \n",
    "#     print(train_set.shape, valid_set.shape)\n",
    "    \n",
    "    return train_set['text'].values, valid_set['text'].values, train_set['category'].values, valid_set['category'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseliner Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_t(y_pred):\n",
    "    return f1_score(y_test, y_pred)\n",
    "\n",
    "def baseliner(X_train, X_valid, y_train, y_valid, X_test, cv=3, metric='f1'):\n",
    "    print(\"Baseliner Models\\n\")\n",
    "    eval_dict = {}\n",
    "    models = [LogisticRegression(), GaussianNB(), lgb.LGBMClassifier(), RandomForestClassifier(),\n",
    "              DecisionTreeClassifier(), ExtraTreeClassifier()\n",
    "             ]\n",
    "    # xgb.XGBClassifier(), GradientBoostingClassifier(), AdaBoostClassifier(), BaggingClassifier(), ExtraTreesClassifier() \n",
    "    print(\"Model Name \\t | Train \\t | Valid \\t | Test \") #    | \\t TRN   | \\t  VAL\n",
    "    print(\"--\" * 30)\n",
    "    \n",
    "    for index, model in enumerate(models, 0):\n",
    "        model_name = str(model).split(\"(\")[0]\n",
    "        eval_dict[model_name] = {}\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        train_f1 = f1_score(y_train, model.predict(X_train))\n",
    "        valid_f1 = f1_score(y_valid, model.predict(X_valid))\n",
    "        test_f1 = eval_t(model.predict(X_test))\n",
    "        \n",
    "        eval_dict[model_name]['train'] = train_f1\n",
    "        eval_dict[model_name]['valid'] = valid_f1\n",
    "        eval_dict[model_name]['test'] = test_f1\n",
    "        \n",
    "        print(\"%s \\t | %.4f \\t | %.4f \\t | %.4f \\t \" % (\n",
    "            model_name[:12], eval_dict[model_name]['train'], eval_dict[model_name]['valid'], eval_dict[model_name]['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s): \n",
    "    return s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_return_vectorizer(train, test, vectorizer, text_col='text', subsample_train=False):\n",
    "\n",
    "    total_texts = pd.concat([train[text_col], test[text_col]], axis=0)\n",
    "    total_texts.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    vectorizer.fit(total_texts.values)\n",
    "    if subsample_train:\n",
    "#         k = np.random.choice(train['fold_id'].unique())\n",
    "        print(f\"Choosing Random Subsample Fold-ID : {K}\")\n",
    "        X_train, X_valid, y_train, y_valid = splitter(train)\n",
    "\n",
    "    train_count_vect = vectorizer.transform(X_train)\n",
    "    valid_count_vect = vectorizer.transform(X_valid)\n",
    "    test_count_vect = vectorizer.transform(test[text_col].values)\n",
    "\n",
    "    print(\"Number of features / words in vocab : {}\".format(len(vectorizer.get_feature_names())))\n",
    "    print(\"Transformed Shapes :: \\nTrain : {}\\nValid : {}\\nTest :  {}\".format(\n",
    "        train_count_vect.shape, valid_count_vect.shape, test_count_vect.shape))\n",
    "    \n",
    "    del total_texts\n",
    "    gc.collect()\n",
    "    \n",
    "    return vectorizer, train_count_vect, valid_count_vect, test_count_vect, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choosing Random Subsample Fold-ID : 6\n",
      "Number of features / words in vocab : 1000\n",
      "Transformed Shapes :: \n",
      "Train : (99371, 1000)\n",
      "Valid : (20593, 1000)\n",
      "Test :  (92, 1000)\n",
      "CPU times: user 30.4 s, sys: 330 ms, total: 30.7 s\n",
      "Wall time: 30.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "max_features = 1_000\n",
    "vect = TfidfVectorizer(max_features=max_features)\n",
    "vect, transformed_train, transformed_valid, transformed_test, y_new_train, y_new_valid = fit_and_return_vectorizer(\n",
    "    train, test, vect, subsample_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseliner Models\n",
      "\n",
      "Model Name \t | Train \t | Valid \t | Test \n",
      "------------------------------------------------------------\n",
      "LogisticRegr \t | 0.9988 \t | 0.9723 \t | 0.9057 \t \n",
      "GaussianNB \t | 0.9807 \t | 0.9618 \t | 0.9358 \t \n",
      "LGBMClassifi \t | 0.9999 \t | 0.9493 \t | 0.8515 \t \n",
      "RandomForest \t | 0.9999 \t | 0.9741 \t | 0.8824 \t \n",
      "DecisionTree \t | 0.9999 \t | 0.9684 \t | 0.8367 \t \n",
      "ExtraTreeCla \t | 0.9999 \t | 0.8586 \t | 0.8515 \t \n"
     ]
    }
   ],
   "source": [
    "baseliner(transformed_train.toarray(), transformed_valid.toarray(), y_new_train, y_new_valid, transformed_test.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix + Understanding Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def plot_classification_report(model, X_train, X_test, y_train, y_test):\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    T = 0.44\n",
    "    preds = model.predict_proba(X_test)[:, 1]\n",
    "    pred_probs = preds.copy()\n",
    "    preds[preds >= T] = 1\n",
    "    preds[preds < T] = 0\n",
    "    \n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    print(\"Classification Report\")\n",
    "    print(classification_report(y_test, preds))\n",
    "    class_names = [\"R\", \"S\"]  # name  of classes\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "    sns.heatmap(pd.DataFrame(cm), annot=True, cmap=\"YlGnBu\", fmt='g')\n",
    "    ax.xaxis.set_label_position(\"top\")\n",
    "    plt.tight_layout()\n",
    "    plt.title('Confusion matrix', y=1.1)\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    \n",
    "    return pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.97      0.91        35\n",
      "           1       0.98      0.89      0.94        57\n",
      "\n",
      "    accuracy                           0.92        92\n",
      "   macro avg       0.92      0.93      0.92        92\n",
      "weighted avg       0.93      0.92      0.92        92\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_probs = plot_classification_report(GaussianNB(), transformed_train.toarray(), transformed_test.toarray(), y_new_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems we have higher False Negatives i.e 6 predictions which are actually Positive which have been faslely classified as Negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does ensembling help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : gaussian_nb\n",
      "Train Score : 0.9807\n",
      "Test Score : 0.9358\n",
      "Model : log_reg\n",
      "Train Score : 0.9988\n",
      "Test Score : 0.9057\n",
      "Model : lgb\n",
      "Train Score : 0.9999\n",
      "Test Score : 0.8515\n"
     ]
    }
   ],
   "source": [
    "good_models = {\n",
    "    \"gaussian_nb\": GaussianNB(),\n",
    "    \"log_reg\" : LogisticRegression(),\n",
    "    \"lgb\" : lgb.LGBMClassifier(),\n",
    "}\n",
    "\n",
    "soft_preds = {}\n",
    "hard_preds = {}\n",
    "\n",
    "for model_name, model in good_models.items():\n",
    "    \n",
    "    print(\"Model : {}\".format(model_name))\n",
    "    hard_preds[model_name] = np.nan\n",
    "    soft_preds[model_name] = np.nan\n",
    "    \n",
    "    model.fit(transformed_train.toarray(), y_new_train)\n",
    "    train_score = f1_score(y_new_train, model.predict(transformed_train.toarray()))\n",
    "    preds = model.predict(transformed_test.toarray())\n",
    "    pred_probs = model.predict_proba(transformed_test.toarray())[:, 1]\n",
    "    \n",
    "    hard_preds[model_name] = preds\n",
    "    soft_preds[model_name] = pred_probs\n",
    "    \n",
    "    test_score = eval_t(preds)\n",
    "    print(\"Train Score : {:.4f}\\nTest Score : {:.4f}\".format(train_score, test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweak_threshold(pred, truth):\n",
    "    thresholds = []\n",
    "    scores = []\n",
    "    for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "        thresh = np.round(thresh, 2)\n",
    "        thresholds.append(thresh)\n",
    "        score = f1_score(truth, (pred>thresh).astype(int))\n",
    "        scores.append(score)\n",
    "    return np.max(scores), thresholds[np.argmax(scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored 0.9999 for threshold 0.41 on train data\n",
      "Scored 0.9259 for threshold 0.41 on test data\n"
     ]
    }
   ],
   "source": [
    "# 2. Soft-Voting\n",
    "# Finding the best threshold for best F1-Score : \n",
    "\n",
    "ens_1 = (soft_preds['gaussian_nb'] + soft_preds['log_reg'] + soft_preds['lgb'])/3\n",
    "\n",
    "# First getting the threshold value from train_predictions\n",
    "score_val, threshold_val = tweak_threshold(model.predict_proba(transformed_train.toarray())[:, 1], y_new_train)\n",
    "print(f\"Scored {round(score_val, 4)} for threshold {threshold_val} on train data\")\n",
    "\n",
    "# Then getting the f1 score in test set using that threshold\n",
    "ens_1[ens_1 >= threshold_val] = 1\n",
    "ens_1[ens_1 < threshold_val] = 0\n",
    "\n",
    "print(f\"Scored {round(eval_t(ens_1), 4)} for threshold {threshold_val} on test data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling - 2 : FlipCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1Evaluation(Callback):\n",
    "    def __init__(self, validation_data=(), test_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "        \n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "        self.X_test, self.y_test = test_data\n",
    "        self.y_val = self.y_val[:, 1]\n",
    "        self.y_test = self.y_test[:, 1]\n",
    "        self.T = 0.5\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_val = self.model.predict(self.X_val, verbose=0)[:, 1]\n",
    "            y_test = self.model.predict(self.X_test, verbose=0)[:, 1]\n",
    "            y_val[y_val >= self.T] = 1\n",
    "            y_val[y_val < self.T] = 0\n",
    "            y_test[y_test >= self.T] = 1\n",
    "            y_test[y_test < self.T] = 0\n",
    "            \n",
    "            val_score = f1_score(self.y_val, y_val)\n",
    "            test_score = f1_score(self.y_test, y_test)\n",
    "            print(\"F1 : Epoch : {} \\t Valid Score : {:.4f} \\t Test Score : {:.4f}\".format(epoch+1, val_score, test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    train = pd.read_csv(\"train_ne.csv\")\n",
    "    test = pd.read_csv(\"test_ne_imputed.csv\")\n",
    "    print(\"Train Shape : {}\\nTest Shape :  {}\".format(train.shape, test.shape))\n",
    "    \n",
    "    train = train[['fold_id', 'title', 'description', 'text', 'category', 'source']]\n",
    "    test = test[['title', 'description', 'text', 'category']]\n",
    "    train.dropna(inplace=True)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape : (1199998, 6)\n",
      "Test Shape :  (92, 5)\n"
     ]
    }
   ],
   "source": [
    "train, test = get_data()\n",
    "target = 'category'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[target] = train[target].map({\"R\": 0, \"S\": 1})\n",
    "train['target_S'] = np.nan\n",
    "train['target_R'] = np.nan\n",
    "train['target_R'] = 1 - train[target].values\n",
    "train['target_S'] = train[target].values\n",
    "\n",
    "train.drop([target], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[target] = test[target].map({\"R\": 0, \"S\": 1})\n",
    "test['target_S'] = np.nan\n",
    "test['target_R'] = np.nan\n",
    "test['target_R'] = 1 - test[target].values\n",
    "test['target_S'] = test[target].values\n",
    "\n",
    "y_test_true = test['target_S'].values\n",
    "test.drop([target], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_classes = [\"target_R\", \"target_S\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Splitter\n",
    "K = 6\n",
    "def splitter(data, k=6):\n",
    "    \n",
    "    data_v1 = data[data['fold_id'] == k].copy()\n",
    "    data_v1.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    train_set = data_v1[data_v1['source'] == 'train'].copy()\n",
    "    valid_set = data_v1[data_v1['source'] == 'valid'].copy()\n",
    "    train_set.reset_index(drop=True, inplace=True)\n",
    "    valid_set.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    del data_v1\n",
    "    gc.collect()\n",
    "\n",
    "    return train_set, valid_set, train_set[list_classes].values, valid_set[list_classes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 0.5\n",
    "def eval_t(y_pred, y_true=y_test_true):\n",
    "    y_pred[y_pred >= T] = 1\n",
    "    y_pred[y_pred < T] = 0\n",
    "    \n",
    "    return f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize word length distribution\n",
    "train['doc_len'] = train['text'].apply(lambda words: len(words.split(\" \")))\n",
    "max_len = np.round(train['doc_len'].mean() + 2*train['doc_len'].std()).astype(int)\n",
    "\n",
    "sns.distplot(train['doc_len'], hist=True, kde=True, color='b', label='doc len')\n",
    "plt.axvline(x=max_len, color='k', linestyle='--', label='max len')\n",
    "plt.title('comment length')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 100\n",
    "max_features = 1_000_000\n",
    "max_len = 50\n",
    "\n",
    "BS = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99371, 8) (20593, 8) (99371, 2) (20593, 2)\n"
     ]
    }
   ],
   "source": [
    "y_test = test[list_classes].values\n",
    "\n",
    "X_train, X_valid, y_new_train, y_new_valid = splitter(train)\n",
    "print(X_train.shape, X_valid.shape, y_new_train.shape, y_new_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1199559/1199559 [00:06<00:00, 192764.34it/s]\n",
      "100%|██████████| 92/92 [00:00<00:00, 64355.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Vocabulary : 47157\n",
      "CPU times: user 4min 2s, sys: 1.69 s, total: 4min 4s\n",
      "Wall time: 2min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "col = 'text'\n",
    "\n",
    "values = train[col].progress_apply(lambda x: x.split()).values.tolist() + test[col].progress_apply(\n",
    "    lambda x: x.split()).values.tolist()\n",
    "embedding_index = Word2Vec(values, min_count=5, size=embed_size)\n",
    "print(\"Length of Vocabulary : {}\".format(len(embedding_index.wv.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_index[\"shirt\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_train = X_train['text'].str.lower()\n",
    "raw_text_valid = X_valid['text'].str.lower()\n",
    "raw_text_test = test['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.82 s, sys: 29.1 ms, total: 6.85 s\n",
      "Wall time: 6.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tk = Tokenizer(num_words=max_features, lower=True)\n",
    "tk.fit_on_texts(raw_text_train.values.tolist() + raw_text_valid.values.tolist() + test['text'].values.tolist())\n",
    "X_train['seq'] = tk.texts_to_sequences(raw_text_train.values)\n",
    "X_valid['seq'] = tk.texts_to_sequences(raw_text_valid.values)\n",
    "test['seq'] = tk.texts_to_sequences(raw_text_test.values)\n",
    "\n",
    "X_train = pad_sequences(X_train.seq, maxlen=max_len)\n",
    "X_valid = pad_sequences(X_valid.seq, maxlen=max_len)\n",
    "test = pad_sequences(test.seq, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec : Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "458c3ebebdb947b9bc1b98611832ad69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=157153.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of words : 157153\n",
      "Coverage : 21.60\n"
     ]
    }
   ],
   "source": [
    "word_index = tk.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "cnt = 0\n",
    "for word, i in tqdm_notebook(word_index.items()):\n",
    "    if i>= nb_words:\n",
    "        continue\n",
    "    try:\n",
    "        embedding_vector = embedding_index[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            cnt += 1\n",
    "    except KeyError:\n",
    "        continue\n",
    "cov = (cnt / nb_words)*100\n",
    "print(\"Number of words : {}\".format(nb_words))\n",
    "print(\"Coverage : {:.2f}\".format(cov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "lr_d = 0\n",
    "units = 128\n",
    "dr = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 5\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_flipcnn_forward(test, embedding_matrix, lr=0.0, lr_d=0.0, units=0, dr=0.0, filename=\"flipcnn_forward\"):\n",
    "    \n",
    "    file_path = \"flipcnn_forward-{epoch:02d}.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor=\"val_loss\", verbose=1, save_best_only=False, save_weights_only=True, mode='auto', period=1)\n",
    "    f1_val = F1Evaluation(validation_data=(X_valid, y_new_valid), test_data=(test, y_test), interval=1)\n",
    "    early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n",
    "    \n",
    "    filter_sizes = [1, 2, 3, 4, 5]\n",
    "    num_filters = 36\n",
    "    \n",
    "    inp = Input(shape=(max_len, ))\n",
    "    x = Embedding(nb_words, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x = SpatialDropout1D(dr)(x)\n",
    "    \n",
    "    conv_0 = Conv1D(num_filters, kernel_size=(filter_sizes[0]), kernel_initializer='he_normal', activation='relu')(x)\n",
    "    conv_1 = Conv1D(num_filters, kernel_size=(filter_sizes[1]), kernel_initializer='he_normal', activation='relu')(x)\n",
    "    conv_2 = Conv1D(num_filters, kernel_size=(filter_sizes[2]), kernel_initializer='he_normal', activation='relu')(x)\n",
    "    conv_3 = Conv1D(num_filters, kernel_size=(filter_sizes[3]), kernel_initializer='he_normal', activation='relu')(x)\n",
    "    \n",
    "    maxpool_0 = MaxPool1D(pool_size=(max_len - filter_sizes[0] + 1))(conv_0)\n",
    "    maxpool_1 = MaxPool1D(pool_size=(max_len - filter_sizes[1] + 1))(conv_1)\n",
    "    maxpool_2 = MaxPool1D(pool_size=(max_len - filter_sizes[2] + 1))(conv_2)\n",
    "    maxpool_3 = MaxPool1D(pool_size=(max_len - filter_sizes[3] + 1))(conv_3)\n",
    "    \n",
    "    z = concatenate([maxpool_0, maxpool_1, maxpool_2, maxpool_3])\n",
    "    z = Flatten()(z)\n",
    "    \n",
    "    outp = Dense(NUM_CLASSES, activation=\"sigmoid\")(z)\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=lr, decay=lr_d), metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_new_train, batch_size=BS, epochs=N_EPOCHS, validation_data=(X_valid, y_new_valid), verbose=1, \n",
    "                        callbacks=[f1_val, check_point, early_stop])\n",
    "    preds = model.predict(test, batch_size=1024, verbose=1)[:, 1]\n",
    "    \n",
    "    return model, preds\n",
    "\n",
    "def build_model_flipcnn_backward(test, embedding_matrix, lr=0.0, lr_d=0.0, units=0, dr=0.0, filename=\"flipcnn_backward\"):\n",
    "    \n",
    "    file_path = \"flipcnn_backward-{epoch:02d}.hdf5\"\n",
    "    check_point = ModelCheckpoint(file_path, monitor=\"val_loss\", verbose=1, save_best_only=False, save_weights_only=True, mode='auto', period=1)\n",
    "    f1_val = F1Evaluation(validation_data=(X_valid, y_new_valid), test_data=(test, y_test), interval=1)\n",
    "    early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n",
    "    \n",
    "    filter_sizes = [1, 2, 3, 4, 5]\n",
    "    num_filters = 36\n",
    "    \n",
    "    inp = Input(shape=(max_len, )) \n",
    "    x = Lambda(lambda x: Ke.reverse(x, axes=-1))(inp)\n",
    "    x = Embedding(nb_words, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x = SpatialDropout1D(dr)(x)\n",
    "    \n",
    "    conv_0 = Conv1D(num_filters, kernel_size=(filter_sizes[0]), kernel_initializer='he_normal', activation='relu')(x)\n",
    "    conv_1 = Conv1D(num_filters, kernel_size=(filter_sizes[1]), kernel_initializer='he_normal', activation='relu')(x)\n",
    "    conv_2 = Conv1D(num_filters, kernel_size=(filter_sizes[2]), kernel_initializer='he_normal', activation='relu')(x)\n",
    "    conv_3 = Conv1D(num_filters, kernel_size=(filter_sizes[3]), kernel_initializer='he_normal', activation='relu')(x)\n",
    "    \n",
    "    maxpool_0 = MaxPool1D(pool_size=(max_len - filter_sizes[0] + 1))(conv_0)\n",
    "    maxpool_1 = MaxPool1D(pool_size=(max_len - filter_sizes[1] + 1))(conv_1)\n",
    "    maxpool_2 = MaxPool1D(pool_size=(max_len - filter_sizes[2] + 1))(conv_2)\n",
    "    maxpool_3 = MaxPool1D(pool_size=(max_len - filter_sizes[3] + 1))(conv_3)\n",
    "    \n",
    "    z = concatenate([maxpool_0, maxpool_1, maxpool_2, maxpool_3])\n",
    "    z = Flatten()(z)\n",
    "    \n",
    "    outp = Dense(NUM_CLASSES, activation=\"sigmoid\")(z)\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=lr, decay=lr_d), metrics=['accuracy'])\n",
    "    history = model.fit(X_train, y_new_train, batch_size=BS, epochs=N_EPOCHS, validation_data=(X_valid, y_new_valid), verbose=1, \n",
    "                        callbacks=[f1_val, check_point, early_stop])\n",
    "    preds = model.predict(test, batch_size=1024, verbose=1)[:, 1]\n",
    "    \n",
    "    return model, preds\n",
    "\n",
    "\n",
    "def build_model_flipcnn(test, embedding_matrix, lr=0.0, lr_d=0.0, units=0, dr=0.0):\n",
    "    model_1, preds_1 = build_model_flipcnn_forward(test, embedding_matrix, lr=lr, lr_d=lr_d, units=units, dr=dr, filename=\"flipcnn_forward\")\n",
    "    model_2, preds_2 = build_model_flipcnn_backward(test, embedding_matrix, lr=lr, lr_d=lr_d, units=units, dr=dr, filename=\"flipcnn_backward\")\n",
    "    \n",
    "    preds = (preds_1 + preds_2)/2\n",
    "    \n",
    "    return model_1, model_2, preds, preds_1, preds_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 99371 samples, validate on 20593 samples\n",
      "Epoch 1/5\n",
      "99371/99371 [==============================] - 6s 56us/step - loss: 0.0203 - accuracy: 0.9940 - val_loss: 0.1647 - val_accuracy: 0.9647\n",
      "F1 : Epoch : 1 \t Valid Score : 0.9783 \t Test Score : 0.9245\n",
      "\n",
      "Epoch 00001: saving model to flipcnn_forward-01.hdf5\n",
      "Epoch 2/5\n",
      "99371/99371 [==============================] - 5s 49us/step - loss: 0.0024 - accuracy: 0.9992 - val_loss: 0.1815 - val_accuracy: 0.9640\n",
      "F1 : Epoch : 2 \t Valid Score : 0.9778 \t Test Score : 0.9143\n",
      "\n",
      "Epoch 00002: saving model to flipcnn_forward-02.hdf5\n",
      "Epoch 3/5\n",
      "99371/99371 [==============================] - 5s 51us/step - loss: 0.0010 - accuracy: 0.9997 - val_loss: 0.1781 - val_accuracy: 0.9639\n",
      "F1 : Epoch : 3 \t Valid Score : 0.9776 \t Test Score : 0.9143\n",
      "\n",
      "Epoch 00003: saving model to flipcnn_forward-03.hdf5\n",
      "Epoch 4/5\n",
      "99371/99371 [==============================] - 5s 49us/step - loss: 4.6507e-04 - accuracy: 0.9999 - val_loss: 0.2495 - val_accuracy: 0.9579\n",
      "F1 : Epoch : 4 \t Valid Score : 0.9695 \t Test Score : 0.9245\n",
      "\n",
      "Epoch 00004: saving model to flipcnn_forward-04.hdf5\n",
      "Epoch 5/5\n",
      "99371/99371 [==============================] - 5s 50us/step - loss: 3.9934e-04 - accuracy: 0.9999 - val_loss: 0.2201 - val_accuracy: 0.9703\n",
      "F1 : Epoch : 5 \t Valid Score : 0.9869 \t Test Score : 0.9143\n",
      "\n",
      "Epoch 00005: saving model to flipcnn_forward-05.hdf5\n",
      "92/92 [==============================] - 0s 110us/step\n",
      "Train on 99371 samples, validate on 20593 samples\n",
      "Epoch 1/5\n",
      "99371/99371 [==============================] - 5s 55us/step - loss: 0.0363 - accuracy: 0.9904 - val_loss: 0.1462 - val_accuracy: 0.9541\n",
      "F1 : Epoch : 1 \t Valid Score : 0.9628 \t Test Score : 0.9245\n",
      "\n",
      "Epoch 00001: saving model to flipcnn_backward-01.hdf5\n",
      "Epoch 2/5\n",
      "99371/99371 [==============================] - 5s 48us/step - loss: 0.0032 - accuracy: 0.9990 - val_loss: 0.1469 - val_accuracy: 0.9560\n",
      "F1 : Epoch : 2 \t Valid Score : 0.9656 \t Test Score : 0.9245\n",
      "\n",
      "Epoch 00002: saving model to flipcnn_backward-02.hdf5\n",
      "Epoch 3/5\n",
      "99371/99371 [==============================] - 5s 52us/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 0.1852 - val_accuracy: 0.9577\n",
      "F1 : Epoch : 3 \t Valid Score : 0.9553 \t Test Score : 0.9245\n",
      "\n",
      "Epoch 00003: saving model to flipcnn_backward-03.hdf5\n",
      "Epoch 4/5\n",
      "99371/99371 [==============================] - 5s 52us/step - loss: 9.9291e-04 - accuracy: 0.9997 - val_loss: 0.1861 - val_accuracy: 0.9574\n",
      "F1 : Epoch : 4 \t Valid Score : 0.9679 \t Test Score : 0.9245\n",
      "\n",
      "Epoch 00004: saving model to flipcnn_backward-04.hdf5\n",
      "Epoch 5/5\n",
      "99371/99371 [==============================] - 5s 54us/step - loss: 5.4070e-04 - accuracy: 0.9998 - val_loss: 0.1525 - val_accuracy: 0.9784\n",
      "F1 : Epoch : 5 \t Valid Score : 0.9850 \t Test Score : 0.9245\n",
      "\n",
      "Epoch 00005: saving model to flipcnn_backward-05.hdf5\n",
      "92/92 [==============================] - 0s 48us/step\n"
     ]
    }
   ],
   "source": [
    "model_fow, model_back, avg_preds, for_preds, back_preds = build_model_flipcnn(test, embedding_matrix, lr=lr, lr_d=lr_d, units=units, dr=dr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9245283018867925, 0.9142857142857143, 0.9245283018867925)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Best epochs indexes : \n",
    "# 1. For : 1 0.9245\n",
    "# 2. Back : 5 0.9245\n",
    "\n",
    "eval_t(avg_preds), eval_t(for_preds), eval_t(back_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flipcnn_forward-01.hdf5\n",
      "92/92 [==============================] - 0s 26us/step\n",
      "flipcnn_backward-05.hdf5\n",
      "92/92 [==============================] - 0s 24us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flipcnn_forward</th>\n",
       "      <th>flipcnn_backward</th>\n",
       "      <th>flipcnn_average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.961477</td>\n",
       "      <td>0.999855</td>\n",
       "      <td>0.980666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.999987</td>\n",
       "      <td>0.999994</td>\n",
       "      <td>0.999991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   flipcnn_forward  flipcnn_backward  flipcnn_average\n",
       "0         0.000015          0.000011         0.000013\n",
       "1         0.961477          0.999855         0.980666\n",
       "2         0.999987          0.999994         0.999991\n",
       "3         1.000000          1.000000         1.000000\n",
       "4         0.000048          0.000006         0.000027"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best epochs indexes \n",
    "\n",
    "sub = pd.DataFrame()\n",
    "best_epoch_dict = {\n",
    "    \"flipcnn_forward\" : [1, model_fow],\n",
    "    \"flipcnn_backward\" : [5, model_back],\n",
    "}\n",
    "avg_preds = np.zeros(for_preds.shape[0])\n",
    "\n",
    "for key, (epoch_id, model) in best_epoch_dict.items():\n",
    "    model_path = \"{}-0{}.hdf5\".format(key, epoch_id)\n",
    "    print(model_path)\n",
    "    model.load_weights(model_path)\n",
    "    preds = model.predict(test, batch_size=1024, verbose=1)[:, 1]\n",
    "    sub[key] = preds\n",
    "    \n",
    "    if key in ['flipcnn_forward', 'flipcnn_backward']:\n",
    "        avg_preds += preds\n",
    "avg_preds /= 2\n",
    "sub['flipcnn_average'] = avg_preds\n",
    "\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_t(sub['flipcnn_average']), eval_t(sub['flipcnn_backward']), eval_t(sub['flipcnn_forward'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test_ne_imputed.csv\")\n",
    "target = 'category'\n",
    "y_test = test[target].map({\"R\": 0, \"S\": 1}).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Ensemble Model Scored 0.9541 on test data\n"
     ]
    }
   ],
   "source": [
    "T = 0.5\n",
    "ens_1 = (((soft_preds['gaussian_nb'] + soft_preds['log_reg'] + soft_preds['lgb'])/3) + (sub['flipcnn_backward']))/2\n",
    "\n",
    "print(\"Best Ensemble Model Scored {} on test data\".format(f1_score(y_test, (ens_1>T).astype(int))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
